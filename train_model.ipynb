{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 4\n",
    "batch_size = 4\n",
    "n_epochs = 5\n",
    "path_best_model = \"objet_model.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dataset = \"U:\\\\PROJET_3A\\\\dataset_objets\" \n",
    "ratio_train_val = 2/3\n",
    "\n",
    "dataset = torchvision.datasets.ImageFolder(\n",
    "    path_dataset,\n",
    "    torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ColorJitter(0.1, 0.1, 0.1, 0.1),\n",
    "        torchvision.transforms.Resize(256),\n",
    "        torchvision.transforms.CenterCrop(224),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    ")\n",
    "\n",
    "n_train = int(ratio_train_val * len(dataset))\n",
    "n_valid = len(dataset) - n_train\n",
    "\n",
    "train_dataset, valid_dataset = torch.utils.data.random_split(dataset, [n_train, n_valid])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] Le chemin d’accès spécifié est introuvable: 'C:\\\\Users\\\\alexi\\\\PROJET_3A\\\\imagenette2-160\\\\train'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-30f49a376a69>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mtorchvision\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mResize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m224\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m224\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mtorchvision\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mtorchvision\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.485\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.456\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.406\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0.229\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.224\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.225\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     ])\n\u001b[0;32m     12\u001b[0m )\n",
      "\u001b[1;32mc:\\APPLIS\\AMESIM\\2020.2\\Amesim\\sys\\python\\win64\\lib\\site-packages\\torchvision\\datasets\\folder.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[0;32m    311\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m                                           \u001b[0mtarget_transform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 313\u001b[1;33m                                           is_valid_file=is_valid_file)\n\u001b[0m\u001b[0;32m    314\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\APPLIS\\AMESIM\\2020.2\\Amesim\\sys\\python\\win64\\lib\\site-packages\\torchvision\\datasets\\folder.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[0;32m    143\u001b[0m         super(DatasetFolder, self).__init__(root, transform=transform,\n\u001b[0;32m    144\u001b[0m                                             target_transform=target_transform)\n\u001b[1;32m--> 145\u001b[1;33m         \u001b[0mclasses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    146\u001b[0m         \u001b[0msamples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\APPLIS\\AMESIM\\2020.2\\Amesim\\sys\\python\\win64\\lib\\site-packages\\torchvision\\datasets\\folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[1;34m(self, directory)\u001b[0m\n\u001b[0;32m    219\u001b[0m             \u001b[1;33m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0meach\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m         \"\"\"\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\APPLIS\\AMESIM\\2020.2\\Amesim\\sys\\python\\win64\\lib\\site-packages\\torchvision\\datasets\\folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[1;34m(directory)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0mSee\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;32mclass\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDatasetFolder\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdetails\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \"\"\"\n\u001b[1;32m---> 40\u001b[1;33m     \u001b[0mclasses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Couldn't find any class folder in {directory}.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] Le chemin d’accès spécifié est introuvable: 'C:\\\\Users\\\\alexi\\\\PROJET_3A\\\\imagenette2-160\\\\train'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "path_train_dataset = \"C:\\\\Users\\\\alexi\\\\PROJET_3A\\\\imagenette2-160\\\\train\"\n",
    "path_valid_dataset = \"C:\\\\Users\\\\alexi\\\\PROJET_3A\\\\imagenette2-160\\\\val\"\n",
    "\n",
    "train_dataset =  torchvision.datasets.ImageFolder(\n",
    "    path_train_dataset,\n",
    "    torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ColorJitter(0.1, 0.1, 0.1, 0.1),\n",
    "        torchvision.transforms.Resize((224, 224)),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    ")\n",
    "\n",
    "valid_dataset = torchvision.datasets.ImageFolder(\n",
    "    path_valid_dataset,\n",
    "    torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ColorJitter(0.1, 0.1, 0.1, 0.1),\n",
    "        torchvision.transforms.Resize((224, 224)),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.alexnet(pretrained=True)\n",
    "model.classifier[6] = torch.nn.Linear(model.classifier[6].in_features, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 batch 1/23\n",
      "epoch 0 batch 2/23\n",
      "epoch 0 batch 3/23\n",
      "epoch 0 batch 4/23\n",
      "epoch 0 batch 5/23\n",
      "epoch 0 batch 6/23\n",
      "epoch 0 batch 7/23\n",
      "epoch 0 batch 8/23\n",
      "epoch 0 batch 9/23\n",
      "epoch 0 batch 10/23\n",
      "epoch 0 batch 11/23\n",
      "epoch 0 batch 12/23\n",
      "epoch 0 batch 13/23\n",
      "epoch 0 batch 14/23\n",
      "epoch 0 batch 15/23\n",
      "epoch 0 batch 16/23\n",
      "epoch 0 batch 17/23\n",
      "epoch 0 batch 18/23\n",
      "epoch 0 batch 19/23\n",
      "epoch 0 batch 20/23\n",
      "epoch 0 batch 21/23\n",
      "epoch 0 batch 22/23\n",
      "epoch 0 batch 23/23\n",
      "test accuracy : 0.787234\n",
      "epoch 1 batch 1/23\n",
      "epoch 1 batch 2/23\n",
      "epoch 1 batch 3/23\n",
      "epoch 1 batch 4/23\n",
      "epoch 1 batch 5/23\n",
      "epoch 1 batch 6/23\n",
      "epoch 1 batch 7/23\n",
      "epoch 1 batch 8/23\n",
      "epoch 1 batch 9/23\n",
      "epoch 1 batch 10/23\n",
      "epoch 1 batch 11/23\n",
      "epoch 1 batch 12/23\n",
      "epoch 1 batch 13/23\n",
      "epoch 1 batch 14/23\n",
      "epoch 1 batch 15/23\n",
      "epoch 1 batch 16/23\n",
      "epoch 1 batch 17/23\n",
      "epoch 1 batch 18/23\n",
      "epoch 1 batch 19/23\n",
      "epoch 1 batch 20/23\n",
      "epoch 1 batch 21/23\n",
      "epoch 1 batch 22/23\n",
      "epoch 1 batch 23/23\n",
      "test accuracy : 0.680851\n",
      "epoch 2 batch 1/23\n",
      "epoch 2 batch 2/23\n",
      "epoch 2 batch 3/23\n",
      "epoch 2 batch 4/23\n",
      "epoch 2 batch 5/23\n",
      "epoch 2 batch 6/23\n",
      "epoch 2 batch 7/23\n",
      "epoch 2 batch 8/23\n",
      "epoch 2 batch 9/23\n",
      "epoch 2 batch 10/23\n",
      "epoch 2 batch 11/23\n",
      "epoch 2 batch 12/23\n",
      "epoch 2 batch 13/23\n",
      "epoch 2 batch 14/23\n",
      "epoch 2 batch 15/23\n",
      "epoch 2 batch 16/23\n",
      "epoch 2 batch 17/23\n",
      "epoch 2 batch 18/23\n",
      "epoch 2 batch 19/23\n",
      "epoch 2 batch 20/23\n",
      "epoch 2 batch 21/23\n",
      "epoch 2 batch 22/23\n",
      "epoch 2 batch 23/23\n",
      "test accuracy : 0.957447\n",
      "epoch 3 batch 1/23\n",
      "epoch 3 batch 2/23\n",
      "epoch 3 batch 3/23\n",
      "epoch 3 batch 4/23\n",
      "epoch 3 batch 5/23\n",
      "epoch 3 batch 6/23\n",
      "epoch 3 batch 7/23\n",
      "epoch 3 batch 8/23\n",
      "epoch 3 batch 9/23\n",
      "epoch 3 batch 10/23\n",
      "epoch 3 batch 11/23\n",
      "epoch 3 batch 12/23\n",
      "epoch 3 batch 13/23\n",
      "epoch 3 batch 14/23\n",
      "epoch 3 batch 15/23\n",
      "epoch 3 batch 16/23\n",
      "epoch 3 batch 17/23\n",
      "epoch 3 batch 18/23\n",
      "epoch 3 batch 19/23\n",
      "epoch 3 batch 20/23\n",
      "epoch 3 batch 21/23\n",
      "epoch 3 batch 22/23\n",
      "epoch 3 batch 23/23\n",
      "test accuracy : 0.978723\n",
      "epoch 4 batch 1/23\n",
      "epoch 4 batch 2/23\n",
      "epoch 4 batch 3/23\n",
      "epoch 4 batch 4/23\n",
      "epoch 4 batch 5/23\n",
      "epoch 4 batch 6/23\n",
      "epoch 4 batch 7/23\n",
      "epoch 4 batch 8/23\n",
      "epoch 4 batch 9/23\n",
      "epoch 4 batch 10/23\n",
      "epoch 4 batch 11/23\n",
      "epoch 4 batch 12/23\n",
      "epoch 4 batch 13/23\n",
      "epoch 4 batch 14/23\n",
      "epoch 4 batch 15/23\n",
      "epoch 4 batch 16/23\n",
      "epoch 4 batch 17/23\n",
      "epoch 4 batch 18/23\n",
      "epoch 4 batch 19/23\n",
      "epoch 4 batch 20/23\n",
      "epoch 4 batch 21/23\n",
      "epoch 4 batch 22/23\n",
      "epoch 4 batch 23/23\n",
      "test accuracy : 1.000000\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "best_accuracy = 0.0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader) :\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = torch.nn.functional.cross_entropy(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print('epoch %d batch %d/%d' % (epoch, (i + 1), len(train_loader)))\n",
    "    test_error_count = 0.0\n",
    "    for images, labels in valid_loader:\n",
    "        outputs = model(images)\n",
    "        test_error_count += float(torch.sum(torch.abs(labels - outputs.argmax(1))))\n",
    "    test_accuracy = 1.0 - float(test_error_count) / float(len(valid_dataset))\n",
    "    print('test accuracy : %f' % test_accuracy)\n",
    "    if test_accuracy > best_accuracy:\n",
    "        torch.save(model.state_dict(), path_best_model)\n",
    "        best_accuracy = test_accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eb3db702564ac2add27c92cea07b321d1c74e5d399bbfc2c476bba8a3af4996b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
